{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "306e9747-2f19-4ac8-9383-b139700c3035",
   "metadata": {},
   "source": [
    "# Project 3: Web APIs & NLP\n",
    "---\n",
    "**Book 1: Data Collection**<br>\n",
    "Book 2: Data Cleaning & Exploratory Data Analysis<br>\n",
    "Book 3: Preprocessing & Vectorization<br>\n",
    "Book 4: ML Modeling<br>\n",
    "Book 5: Sentiment Analysis, Conclusion & Recommendation<br>\n",
    "Author: Lee Wan Xian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd27454c-188f-4493-9cbd-5aa45cf170ed",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c0b820-fd75-4f5f-bb3e-3326a1adb3d7",
   "metadata": {},
   "source": [
    "Our client is a firm that runs a streaming service discussion website. As part of their initiative to build an inhouse label tagging algorithm, they have tasked us to develop a machine learning (ML) classification model that tags posts to the right streaming service tag. Meanwhile, the client is also interested in users' sentiments towards famous shows. That way, they can evaluate how to improve their search and homepage recommendations for users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976b879b-3819-48a6-b478-54ec5448f473",
   "metadata": {},
   "source": [
    "## Contents:\n",
    "- [Background](#Background)\n",
    "- [Data Collection](#Data-Collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0906c3b1-6cb0-4d39-81d9-7dc2111e5dcb",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdb78a2-2b5a-4ec7-a623-009853e31b56",
   "metadata": {},
   "source": [
    "There are over 200 streaming services globally ([source](https://flixed.io/complete-list-streaming-services/)). With users having so many options in streaming services, the forum website that the client runs has become a hotspot for open discussions and sharing. The client wish to enable label tags onto posts so that users can use it to search for forum posts related to specific streaming service. Since this change is new to the firm, they have engaged us to build a ML classification model that can perform such a task.\n",
    "\n",
    "As a team of data professionals, we will leverage on Reddit to form the training data for our model. This is because the client lacks the resources for human annotation on their own forum posts. Reddit will serve as a good substitute to the training data, given the similar nature of business, similar user demographics and the fact that there is a way to differentiate the streaming service from reddit posts.\n",
    "\n",
    "For the purpose of this project, we will perform webscraping on the below subreddits using [Pushshift's](https://github.com/pushshift/api) API.\n",
    "* `r/DisneyPlus`: https://www.reddit.com/r/DisneyPlus/\n",
    "* `r/netflix`: https://www.reddit.com/r/netflix/\n",
    "\n",
    "To add on, the client also wish to understand users' sentiments on famous shows. With a good understanding on user's sentiments, they can leverage it to improve on their search and homepage recommendations. In turn, improving users' experience with the website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3152e89-61bb-4557-9bbe-7c13fdb9a291",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42788f2-d4d8-48da-8822-5edca990fada",
   "metadata": {},
   "source": [
    "## Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3922a485-d120-44e8-898b-09a24d92e05b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d29f7d-2b72-4c04-8b51-717a0ee8a435",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aee5f36-2ca2-49cc-ad30-7f70347705c8",
   "metadata": {},
   "source": [
    "For this project, we will be extracting 15,000 reddit posts from each subreddit (`r/DisneyPlus` and `r/netflix`). The posts extracted were posted on September 30, 2022 11:59 PM GMT time or earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6b43a70-6af1-4e80-aa87-6aeccc23d4dd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to webscrape posts from subreddit into dataframe\n",
    "\n",
    "def reddit_to_df(reddit, runs, post_count=150, before=1664582340):\n",
    "    \n",
    "    url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "    params = {'subreddit': reddit, 'size': post_count, 'before': before}\n",
    "    posts = []\n",
    "    \n",
    "    for i in range(runs):\n",
    "        res = requests.get(url, params)\n",
    "        \n",
    "        if res.status_code != 200:\n",
    "            print(f'ERROR: Unable to scrape from the subreddit due to HTML Status code {res.status_code}')\n",
    "        else:\n",
    "            reddit_sub = res.json()\n",
    "            posts += reddit_sub['data']\n",
    "            params['before'] = posts[-1]['created_utc']\n",
    "            print(f'Batch {i+1} scraped into Dataframe, earliest created_utc in this batch: {posts[-1][\"created_utc\"]}')\n",
    "            time.sleep(3)\n",
    "            \n",
    "    df = pd.DataFrame(posts)\n",
    "    \n",
    "    # with reference to pushshift API docs, these 5 columns should provide enough value & insights to the classification model & sentiment analysis\n",
    "    return df[['subreddit','title','selftext','is_video','created_utc']] \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e88a2e97-f3c3-473c-992c-9d90d6b025ee",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 scraped into Dataframe, earliest created_utc in this batch: 1664002270\n",
      "Batch 2 scraped into Dataframe, earliest created_utc in this batch: 1663363347\n",
      "Batch 3 scraped into Dataframe, earliest created_utc in this batch: 1662839944\n",
      "Batch 4 scraped into Dataframe, earliest created_utc in this batch: 1662388926\n",
      "Batch 5 scraped into Dataframe, earliest created_utc in this batch: 1661450795\n",
      "Batch 6 scraped into Dataframe, earliest created_utc in this batch: 1660798533\n",
      "Batch 7 scraped into Dataframe, earliest created_utc in this batch: 1660188020\n",
      "Batch 8 scraped into Dataframe, earliest created_utc in this batch: 1659689701\n",
      "Batch 9 scraped into Dataframe, earliest created_utc in this batch: 1659060446\n",
      "Batch 10 scraped into Dataframe, earliest created_utc in this batch: 1658538331\n",
      "Batch 11 scraped into Dataframe, earliest created_utc in this batch: 1658081126\n",
      "Batch 12 scraped into Dataframe, earliest created_utc in this batch: 1657469367\n",
      "Batch 13 scraped into Dataframe, earliest created_utc in this batch: 1656857522\n",
      "Batch 14 scraped into Dataframe, earliest created_utc in this batch: 1656348310\n",
      "Batch 15 scraped into Dataframe, earliest created_utc in this batch: 1655902997\n",
      "Batch 16 scraped into Dataframe, earliest created_utc in this batch: 1655498229\n",
      "Batch 17 scraped into Dataframe, earliest created_utc in this batch: 1655206979\n",
      "Batch 18 scraped into Dataframe, earliest created_utc in this batch: 1654769528\n",
      "Batch 19 scraped into Dataframe, earliest created_utc in this batch: 1654192409\n",
      "Batch 20 scraped into Dataframe, earliest created_utc in this batch: 1653681545\n",
      "Batch 21 scraped into Dataframe, earliest created_utc in this batch: 1653277459\n",
      "Batch 22 scraped into Dataframe, earliest created_utc in this batch: 1652824877\n",
      "Batch 23 scraped into Dataframe, earliest created_utc in this batch: 1652218774\n",
      "Batch 24 scraped into Dataframe, earliest created_utc in this batch: 1651481134\n",
      "Batch 25 scraped into Dataframe, earliest created_utc in this batch: 1650935826\n",
      "Batch 26 scraped into Dataframe, earliest created_utc in this batch: 1650391816\n",
      "Batch 27 scraped into Dataframe, earliest created_utc in this batch: 1649776016\n",
      "Batch 28 scraped into Dataframe, earliest created_utc in this batch: 1649112352\n",
      "Batch 29 scraped into Dataframe, earliest created_utc in this batch: 1648646599\n",
      "Batch 30 scraped into Dataframe, earliest created_utc in this batch: 1648218475\n",
      "Batch 31 scraped into Dataframe, earliest created_utc in this batch: 1647734460\n",
      "Batch 32 scraped into Dataframe, earliest created_utc in this batch: 1647284960\n",
      "Batch 33 scraped into Dataframe, earliest created_utc in this batch: 1646789920\n",
      "Batch 34 scraped into Dataframe, earliest created_utc in this batch: 1646098014\n",
      "Batch 35 scraped into Dataframe, earliest created_utc in this batch: 1645427848\n",
      "Batch 36 scraped into Dataframe, earliest created_utc in this batch: 1644778244\n",
      "Batch 37 scraped into Dataframe, earliest created_utc in this batch: 1644265871\n",
      "Batch 38 scraped into Dataframe, earliest created_utc in this batch: 1643665618\n",
      "Batch 39 scraped into Dataframe, earliest created_utc in this batch: 1642944293\n",
      "Batch 40 scraped into Dataframe, earliest created_utc in this batch: 1642376755\n",
      "Batch 41 scraped into Dataframe, earliest created_utc in this batch: 1641943868\n",
      "Batch 42 scraped into Dataframe, earliest created_utc in this batch: 1641408386\n",
      "Batch 43 scraped into Dataframe, earliest created_utc in this batch: 1640860046\n",
      "Batch 44 scraped into Dataframe, earliest created_utc in this batch: 1640350780\n",
      "Batch 45 scraped into Dataframe, earliest created_utc in this batch: 1639618589\n",
      "Batch 46 scraped into Dataframe, earliest created_utc in this batch: 1638984255\n",
      "Batch 47 scraped into Dataframe, earliest created_utc in this batch: 1638337500\n",
      "Batch 48 scraped into Dataframe, earliest created_utc in this batch: 1637777614\n",
      "Batch 49 scraped into Dataframe, earliest created_utc in this batch: 1637138385\n",
      "Batch 50 scraped into Dataframe, earliest created_utc in this batch: 1636802992\n",
      "Batch 51 scraped into Dataframe, earliest created_utc in this batch: 1636681242\n",
      "Batch 52 scraped into Dataframe, earliest created_utc in this batch: 1636303739\n",
      "Batch 53 scraped into Dataframe, earliest created_utc in this batch: 1635733292\n",
      "Batch 54 scraped into Dataframe, earliest created_utc in this batch: 1634896310\n",
      "Batch 55 scraped into Dataframe, earliest created_utc in this batch: 1634242870\n",
      "Batch 56 scraped into Dataframe, earliest created_utc in this batch: 1633621279\n",
      "Batch 57 scraped into Dataframe, earliest created_utc in this batch: 1633170814\n",
      "Batch 58 scraped into Dataframe, earliest created_utc in this batch: 1632745839\n",
      "Batch 59 scraped into Dataframe, earliest created_utc in this batch: 1632322364\n",
      "Batch 60 scraped into Dataframe, earliest created_utc in this batch: 1631662983\n",
      "Batch 61 scraped into Dataframe, earliest created_utc in this batch: 1631167134\n",
      "Batch 62 scraped into Dataframe, earliest created_utc in this batch: 1630648012\n",
      "Batch 63 scraped into Dataframe, earliest created_utc in this batch: 1630237313\n",
      "Batch 64 scraped into Dataframe, earliest created_utc in this batch: 1629730906\n",
      "Batch 65 scraped into Dataframe, earliest created_utc in this batch: 1629074798\n",
      "Batch 66 scraped into Dataframe, earliest created_utc in this batch: 1628618144\n",
      "Batch 67 scraped into Dataframe, earliest created_utc in this batch: 1628092797\n",
      "Batch 68 scraped into Dataframe, earliest created_utc in this batch: 1627593681\n",
      "Batch 69 scraped into Dataframe, earliest created_utc in this batch: 1627051613\n",
      "Batch 70 scraped into Dataframe, earliest created_utc in this batch: 1626611651\n",
      "Batch 71 scraped into Dataframe, earliest created_utc in this batch: 1626237862\n",
      "Batch 72 scraped into Dataframe, earliest created_utc in this batch: 1625853061\n",
      "Batch 73 scraped into Dataframe, earliest created_utc in this batch: 1625489706\n",
      "Batch 74 scraped into Dataframe, earliest created_utc in this batch: 1625074346\n",
      "Batch 75 scraped into Dataframe, earliest created_utc in this batch: 1624522132\n",
      "Batch 76 scraped into Dataframe, earliest created_utc in this batch: 1624050987\n",
      "Batch 77 scraped into Dataframe, earliest created_utc in this batch: 1623806950\n",
      "Batch 78 scraped into Dataframe, earliest created_utc in this batch: 1623290455\n",
      "Batch 79 scraped into Dataframe, earliest created_utc in this batch: 1622818788\n",
      "Batch 80 scraped into Dataframe, earliest created_utc in this batch: 1622389360\n",
      "Batch 81 scraped into Dataframe, earliest created_utc in this batch: 1621871158\n",
      "Batch 82 scraped into Dataframe, earliest created_utc in this batch: 1621350459\n",
      "Batch 83 scraped into Dataframe, earliest created_utc in this batch: 1620929830\n",
      "Batch 84 scraped into Dataframe, earliest created_utc in this batch: 1620522206\n",
      "Batch 85 scraped into Dataframe, earliest created_utc in this batch: 1620171313\n",
      "Batch 86 scraped into Dataframe, earliest created_utc in this batch: 1619807373\n",
      "Batch 87 scraped into Dataframe, earliest created_utc in this batch: 1619458753\n",
      "Batch 88 scraped into Dataframe, earliest created_utc in this batch: 1619040601\n",
      "Batch 89 scraped into Dataframe, earliest created_utc in this batch: 1618657109\n",
      "Batch 90 scraped into Dataframe, earliest created_utc in this batch: 1617887279\n",
      "Batch 91 scraped into Dataframe, earliest created_utc in this batch: 1617483880\n",
      "Batch 92 scraped into Dataframe, earliest created_utc in this batch: 1617179966\n",
      "Batch 93 scraped into Dataframe, earliest created_utc in this batch: 1616888666\n",
      "Batch 94 scraped into Dataframe, earliest created_utc in this batch: 1615674103\n",
      "Batch 95 scraped into Dataframe, earliest created_utc in this batch: 1615319140\n",
      "Batch 96 scraped into Dataframe, earliest created_utc in this batch: 1614825628\n",
      "Batch 97 scraped into Dataframe, earliest created_utc in this batch: 1614389593\n",
      "Batch 98 scraped into Dataframe, earliest created_utc in this batch: 1614226166\n",
      "Batch 99 scraped into Dataframe, earliest created_utc in this batch: 1614099533\n",
      "Batch 100 scraped into Dataframe, earliest created_utc in this batch: 1614041482\n"
     ]
    }
   ],
   "source": [
    "# Scrape 15_000 reddit posts from DisneyPlus subreddit into dataframe\n",
    "\n",
    "dfdisney_raw = reddit_to_df(reddit='DisneyPlus', runs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89844771-3b15-4a29-a5a8-22b28816a31e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 scraped into Dataframe, earliest created_utc in this batch: 1664369608\n",
      "Batch 2 scraped into Dataframe, earliest created_utc in this batch: 1664179472\n",
      "Batch 3 scraped into Dataframe, earliest created_utc in this batch: 1664020243\n",
      "Batch 4 scraped into Dataframe, earliest created_utc in this batch: 1663788929\n",
      "Batch 5 scraped into Dataframe, earliest created_utc in this batch: 1663436282\n",
      "Batch 6 scraped into Dataframe, earliest created_utc in this batch: 1663171219\n",
      "Batch 7 scraped into Dataframe, earliest created_utc in this batch: 1662882024\n",
      "Batch 8 scraped into Dataframe, earliest created_utc in this batch: 1662741635\n",
      "Batch 9 scraped into Dataframe, earliest created_utc in this batch: 1662535441\n",
      "Batch 10 scraped into Dataframe, earliest created_utc in this batch: 1662207440\n",
      "Batch 11 scraped into Dataframe, earliest created_utc in this batch: 1661886731\n",
      "Batch 12 scraped into Dataframe, earliest created_utc in this batch: 1661617444\n",
      "Batch 13 scraped into Dataframe, earliest created_utc in this batch: 1661366917\n",
      "Batch 14 scraped into Dataframe, earliest created_utc in this batch: 1661120901\n",
      "Batch 15 scraped into Dataframe, earliest created_utc in this batch: 1660852649\n",
      "Batch 16 scraped into Dataframe, earliest created_utc in this batch: 1660583579\n",
      "Batch 17 scraped into Dataframe, earliest created_utc in this batch: 1660351973\n",
      "Batch 18 scraped into Dataframe, earliest created_utc in this batch: 1660129461\n",
      "Batch 19 scraped into Dataframe, earliest created_utc in this batch: 1659907392\n",
      "Batch 20 scraped into Dataframe, earliest created_utc in this batch: 1659638799\n",
      "Batch 21 scraped into Dataframe, earliest created_utc in this batch: 1659412586\n",
      "Batch 22 scraped into Dataframe, earliest created_utc in this batch: 1659168889\n",
      "Batch 23 scraped into Dataframe, earliest created_utc in this batch: 1658877863\n",
      "Batch 24 scraped into Dataframe, earliest created_utc in this batch: 1658675944\n",
      "Batch 25 scraped into Dataframe, earliest created_utc in this batch: 1658646817\n",
      "Batch 26 scraped into Dataframe, earliest created_utc in this batch: 1658411638\n",
      "Batch 27 scraped into Dataframe, earliest created_utc in this batch: 1658187019\n",
      "Batch 28 scraped into Dataframe, earliest created_utc in this batch: 1657986673\n",
      "Batch 29 scraped into Dataframe, earliest created_utc in this batch: 1657757787\n",
      "Batch 30 scraped into Dataframe, earliest created_utc in this batch: 1657463750\n",
      "Batch 31 scraped into Dataframe, earliest created_utc in this batch: 1657232954\n",
      "Batch 32 scraped into Dataframe, earliest created_utc in this batch: 1656962648\n",
      "Batch 33 scraped into Dataframe, earliest created_utc in this batch: 1656746022\n",
      "Batch 34 scraped into Dataframe, earliest created_utc in this batch: 1656542671\n",
      "Batch 35 scraped into Dataframe, earliest created_utc in this batch: 1656275474\n",
      "Batch 36 scraped into Dataframe, earliest created_utc in this batch: 1656036990\n",
      "Batch 37 scraped into Dataframe, earliest created_utc in this batch: 1655778845\n",
      "Batch 38 scraped into Dataframe, earliest created_utc in this batch: 1655539801\n",
      "Batch 39 scraped into Dataframe, earliest created_utc in this batch: 1655265913\n",
      "Batch 40 scraped into Dataframe, earliest created_utc in this batch: 1655010663\n",
      "Batch 41 scraped into Dataframe, earliest created_utc in this batch: 1654732680\n",
      "Batch 42 scraped into Dataframe, earliest created_utc in this batch: 1654499996\n",
      "Batch 43 scraped into Dataframe, earliest created_utc in this batch: 1654182255\n",
      "Batch 44 scraped into Dataframe, earliest created_utc in this batch: 1653944179\n",
      "Batch 45 scraped into Dataframe, earliest created_utc in this batch: 1653698729\n",
      "Batch 46 scraped into Dataframe, earliest created_utc in this batch: 1653427601\n",
      "Batch 47 scraped into Dataframe, earliest created_utc in this batch: 1653170101\n",
      "Batch 48 scraped into Dataframe, earliest created_utc in this batch: 1652891423\n",
      "Batch 49 scraped into Dataframe, earliest created_utc in this batch: 1652602704\n",
      "Batch 50 scraped into Dataframe, earliest created_utc in this batch: 1652280283\n",
      "Batch 51 scraped into Dataframe, earliest created_utc in this batch: 1651989027\n",
      "Batch 52 scraped into Dataframe, earliest created_utc in this batch: 1651730521\n",
      "Batch 53 scraped into Dataframe, earliest created_utc in this batch: 1651493550\n",
      "Batch 54 scraped into Dataframe, earliest created_utc in this batch: 1651236361\n",
      "Batch 55 scraped into Dataframe, earliest created_utc in this batch: 1650979790\n",
      "Batch 56 scraped into Dataframe, earliest created_utc in this batch: 1650771268\n",
      "Batch 57 scraped into Dataframe, earliest created_utc in this batch: 1650626181\n",
      "Batch 58 scraped into Dataframe, earliest created_utc in this batch: 1650508798\n",
      "Batch 59 scraped into Dataframe, earliest created_utc in this batch: 1650400049\n",
      "Batch 60 scraped into Dataframe, earliest created_utc in this batch: 1650078502\n",
      "Batch 61 scraped into Dataframe, earliest created_utc in this batch: 1649787986\n",
      "Batch 62 scraped into Dataframe, earliest created_utc in this batch: 1649539742\n",
      "Batch 63 scraped into Dataframe, earliest created_utc in this batch: 1649241841\n",
      "Batch 64 scraped into Dataframe, earliest created_utc in this batch: 1648922033\n",
      "Batch 65 scraped into Dataframe, earliest created_utc in this batch: 1648646007\n",
      "Batch 66 scraped into Dataframe, earliest created_utc in this batch: 1648316183\n",
      "Batch 67 scraped into Dataframe, earliest created_utc in this batch: 1648076394\n",
      "Batch 68 scraped into Dataframe, earliest created_utc in this batch: 1647838974\n",
      "Batch 69 scraped into Dataframe, earliest created_utc in this batch: 1647640296\n",
      "Batch 70 scraped into Dataframe, earliest created_utc in this batch: 1647424083\n",
      "Batch 71 scraped into Dataframe, earliest created_utc in this batch: 1647133065\n",
      "Batch 72 scraped into Dataframe, earliest created_utc in this batch: 1646858878\n",
      "Batch 73 scraped into Dataframe, earliest created_utc in this batch: 1646625654\n",
      "Batch 74 scraped into Dataframe, earliest created_utc in this batch: 1646386730\n",
      "Batch 75 scraped into Dataframe, earliest created_utc in this batch: 1646143768\n",
      "Batch 76 scraped into Dataframe, earliest created_utc in this batch: 1646027632\n",
      "Batch 77 scraped into Dataframe, earliest created_utc in this batch: 1645811238\n",
      "Batch 78 scraped into Dataframe, earliest created_utc in this batch: 1645523052\n",
      "Batch 79 scraped into Dataframe, earliest created_utc in this batch: 1645274601\n",
      "Batch 80 scraped into Dataframe, earliest created_utc in this batch: 1645032612\n",
      "Batch 81 scraped into Dataframe, earliest created_utc in this batch: 1644788812\n",
      "Batch 82 scraped into Dataframe, earliest created_utc in this batch: 1644575597\n",
      "Batch 83 scraped into Dataframe, earliest created_utc in this batch: 1644320210\n",
      "Batch 84 scraped into Dataframe, earliest created_utc in this batch: 1644156649\n",
      "Batch 85 scraped into Dataframe, earliest created_utc in this batch: 1644100183\n",
      "Batch 86 scraped into Dataframe, earliest created_utc in this batch: 1643889511\n",
      "Batch 87 scraped into Dataframe, earliest created_utc in this batch: 1643619469\n",
      "Batch 88 scraped into Dataframe, earliest created_utc in this batch: 1643405048\n",
      "Batch 89 scraped into Dataframe, earliest created_utc in this batch: 1643218884\n",
      "Batch 90 scraped into Dataframe, earliest created_utc in this batch: 1642952039\n",
      "Batch 91 scraped into Dataframe, earliest created_utc in this batch: 1642707013\n",
      "Batch 92 scraped into Dataframe, earliest created_utc in this batch: 1642451412\n",
      "Batch 93 scraped into Dataframe, earliest created_utc in this batch: 1642265235\n",
      "Batch 94 scraped into Dataframe, earliest created_utc in this batch: 1642018953\n",
      "Batch 95 scraped into Dataframe, earliest created_utc in this batch: 1641787163\n",
      "Batch 96 scraped into Dataframe, earliest created_utc in this batch: 1641555999\n",
      "Batch 97 scraped into Dataframe, earliest created_utc in this batch: 1641304820\n",
      "Batch 98 scraped into Dataframe, earliest created_utc in this batch: 1641119593\n",
      "Batch 99 scraped into Dataframe, earliest created_utc in this batch: 1640912053\n",
      "Batch 100 scraped into Dataframe, earliest created_utc in this batch: 1640720030\n"
     ]
    }
   ],
   "source": [
    "# Scrape 15_000 reddit posts from netflix subreddit into dataframe\n",
    "\n",
    "dfnetflix_raw = reddit_to_df(reddit='netflix', runs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f2f1aea-ce80-404c-93ab-57eb415fe62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The no. of rows,columns in DisneyPlus subreddit corpus is (14980, 5).\n",
      "The no. of rows,columns in Netflix subreddit corpus is (14990, 5).\n"
     ]
    }
   ],
   "source": [
    "# Show the shape for both DisneyPlus & Netflix subreddit\n",
    "print(f'The no. of rows,columns in DisneyPlus subreddit corpus is {dfdisney_raw.shape}.')\n",
    "print(f'The no. of rows,columns in Netflix subreddit corpus is {dfnetflix_raw.shape}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520e3c71-b7b1-4679-8fcb-acdc82caa938",
   "metadata": {},
   "source": [
    "A total of 29970 posts were extracted from both `r/DisneyPlus` and `r/netflix` with a time frame spanning from Unix Epoch 1614041482 to 1664582340. Only `subreddit`, `title`, `selftext`, `is_video` and `created_utc` fields of the posts were extracted for modelling and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae6a1872-fd89-49fd-a99d-dab5965c242c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine both dataframes into 1 for EDA & Modelling\n",
    "\n",
    "df_raw = pd.concat([dfdisney_raw, dfnetflix_raw])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb0c1d8-3f00-424e-aca9-a4bae8561a78",
   "metadata": {},
   "source": [
    "### Export the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38e1afe6-2380-44c6-bdfd-53a0557cb3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# create new folder named 'data' if it does not exist\n",
    "if not os.path.exists('../data'):\n",
    "    os.makedirs('../data')\n",
    "    \n",
    "# Export the dataframes into csv files    \n",
    "df_raw.to_csv('../data/df_raw.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac68fd5a-5463-4805-baad-a817e6ca3734",
   "metadata": {},
   "source": [
    "**Please proceed to Book 2 for Data Cleaning & EDA.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi-proj3]",
   "language": "python",
   "name": "conda-env-dsi-proj3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
